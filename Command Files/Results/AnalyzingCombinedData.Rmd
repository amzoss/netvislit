---
title: "Analyzing Combined NetVisLit Data"
author: "Angela Zoss"
date: "July 19, 2017"
output: github_document
---

Notes:

These command files should contain commands that open up your analysis data files, and then use those data to generate the output upon which your results are based.

Every command that generates any of your results should be preceded by a comment that states which result the command generates.  A few Hypothetical examples illustrate what these comments might look like:

* The following command generates the first column of Table 6.

The command files for your analysis phase should not contain any commands that generate new variables or process your data in any way.  All the procedures required to prepare your data for analysis should be executed by the command files you wrote for the processing phase.

It is often convenient to write all the commands for the analysis phase in a single command file. However, if the nature of your project or the structure of your data are such that you think it would make sense to divide the code that generates the results into two or more command files, you should feel free to do so.  No matter how you organize your analysis command files, your Read Me file will include an explanation of how to use them to reproduce your results.

Save the command files you write for the analysis phase in the Command Files folder.


## Load packages

```{r}

require(tidyverse)
library(officer)
library(rvg)
library(devEMF)
library(nlme)
library(lme4)
library(car)

```

## Environmental Variables

```{r}

originalDataDir <- "../../Original Data"
analysisDataDir <- "../../Analysis Data"

generatedDataDir <- file.path(originalDataDir, "Generated data")

figureDir <- "../../Documents/"

```

## Loading analysis data files

```{r}

#all_nodes <- read_csv(file.path(generatedDataDir, "all_nodes.csv"))

node_lookup <- read_csv(file.path(generatedDataDir, "node_lookup.csv"), col_types = cols(MaxValue = col_double(),NodeValue = col_double()))

graded_num_ans <- read_csv(file.path(analysisDataDir, "GradedNumAnswers.csv"))

graded_nodes <- read_csv(file.path(analysisDataDir, "GradedNodes.csv"))

stats_datasets_tall <- read_csv(file.path(analysisDataDir, "Stats_Datasets_Tall.csv"))

stats_demo <- read_csv(file.path(analysisDataDir, "Stats_Demo.csv"))

responses <- read_csv(file.path(analysisDataDir, "CombinedResponsesWithOrder.csv"))

num_ans_lookup <- read_csv(file.path(generatedDataDir, "numerical_answer_lookup.csv"))

#responses <- read_csv(file.path(analysisDataDir, "Pilot3ResponsesWithOrder.csv"))

exp_only <- read_csv(file.path(analysisDataDir, "ExpOnly.csv"))

```

## Slight processing for analysis

```{r}

graded_num_ans$ClustConf <- factor(graded_num_ans$ClustConf, levels = c("Very doubtful (0-25%)","Somewhat doubtful (26-50%)","Somewhat confident (51-75%)","Very confident (76-100%)"), ordered = TRUE)

graded_num_ans$Condition <- factor(graded_num_ans$Condition,
                                   levels=c("Ctrl","Phr","Siz","Col","Fru","Ord","Cir"))

graded_nodes$Condition <- factor(graded_nodes$Condition,
                                   levels=c("Ctrl","Phr","Siz","Col","Fru","Ord","Cir"))


stats_demo$`Stats-Group` <- factor(stats_demo$`Stats-Group`,
                                   levels=c("control", "phrasing", "size", "color", 
                                            "frucht", "openord", "circle"))

stats_demo$`Demo-educ` <- factor(stats_demo$`Demo-educ`, 
                                 levels=c("did not complete High School diploma",
                                          "High School diploma",
                                          "Bachelor’s degree",
                                          "Professional degree",
                                          "Master’s degree",
                                          "Doctorate degree",
                                          "Other"))

freq4 <- c("None", "A little", "Some", "A lot")

stats_demo$`Demo-expdataanal` <- factor(stats_demo$`Demo-expdataanal`, 
                                        levels = freq4, 
                                        ordered = TRUE)

stats_demo$`Demo-expdatavis` <- factor(stats_demo$`Demo-expdatavis`, 
                                        levels = freq4, 
                                        ordered = TRUE)

stats_demo$`Demo-expreadnetvis` <- factor(stats_demo$`Demo-expreadnetvis`, 
                                        levels = freq4, 
                                        ordered = TRUE)

stats_demo$`Demo-expcreatenetvis` <- factor(stats_demo$`Demo-expcreatenetvis`, 
                                        levels = freq4, 
                                        ordered = TRUE)



```

```{r}

graded_num_ans <- left_join(graded_num_ans, stats_demo)
graded_num_ans <- left_join(graded_num_ans, stats_datasets_tall)

graded_nodes <- left_join(graded_nodes, stats_demo)
graded_nodes <- left_join(graded_nodes, stats_datasets_tall)

```

## Summarize Stats and Demo data

```{r}

ggplot(stats_demo) +
  geom_density(aes(`Stats-Q_TotalDuration` / 60, color=filename)) +
#  geom_histogram(aes(`Stats-Q_TotalDuration` / 60)) +
#  geom_dotplot(aes(x = (`Stats-Q_TotalDuration` / 60)), binwidth = 3) +
  scale_x_continuous(limits = c(0,70), name = "Total Duration in Minutes") +
  #facet_grid(filename~.)
  facet_grid(NetVisExperience~.)
  


```
```{r}

ggplot(stats_demo) +
  geom_bin2d(aes(x = factor(`Stats-dataset_count`), y=filename))


```

```{r}

ggplot(stats_demo) +
#  geom_histogram(aes(`Stats-Q_TotalDuration`)) +
  geom_dotplot(aes(x = (`Stats-Q_TotalDuration` / `Stats-dataset_count` / 60)), binwidth = 1) +
  scale_x_continuous(limits = c(0,25), name = "Average duration in minutes per dataset (bins = 1 minute each)") +
  facet_grid(filename~.)


```

```{r}

ggplot(stats_demo) +
  geom_boxplot(aes(x = NetVisExperience, y = (`Stats-Q_TotalDuration` / `Stats-dataset_count` / 60), color=filename)) +
  labs(y="Avg. Duration per dataset in minutes\n(filtered to exclude averages over 50 min.)") +
  scale_y_continuous(limits = c(0,50))

```

```{r}

ggplot(stats_datasets_tall) +
  geom_boxplot(aes(x = NetVisExperience, y = (DatasetDuration / 60), color=factor(DatasetOrder))) + 
  scale_y_continuous(limits=c(0,125)) #+
  #facet_grid(NetVisExperience~.)


```

```{r}

three.block.duration <- stats_datasets_tall %>% filter(filename=="PilotStudents") %>% group_by(`Demo-ResponseID`) %>% summarise(totalDuration=sum(DatasetDuration)/60) %>% dplyr::select(`Demo-ResponseID`,totalDuration)

two.block.duration <- stats_datasets_tall %>% filter(filename=="PilotStudents") %>% filter(DatasetOrder < 3) %>% group_by(`Demo-ResponseID`) %>% summarise(totalDuration=sum(DatasetDuration)/60) %>% dplyr::select(`Demo-ResponseID`,totalDuration)
  
ggplot(two.block.duration) + geom_boxplot(aes(y=totalDuration,x="all"))
```

```{r}
median(two.block.duration$totalDuration)

median(three.block.duration$totalDuration)

median(stats_demo$`Stats-Q_TotalDuration`/60)

```


```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Stats-Group`)) +
  facet_grid(NetVisExperience~.)


```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Stats-BrowserName`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Stats-OperatingSystem`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Stats-ScreenResolution`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_dotplot(aes(`Demo-age`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-gender`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-lang`)) +
  facet_grid(filename~.)

```

```{r}

table(stats_demo$`Demo-lang_TEXT`, stats_demo$filename)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-educ`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-acfield`)) +
  coord_flip() +
  facet_grid(filename~.)

```

```{r}

table(stats_demo$`Demo-acfieldother`, stats_demo$filename)

```

```{r}

#stats_demo$`Demo-dailytech_Tablet` <- type.convert(sub(-99, 0, stats_demo$`Demo-dailytech_Tablet`))

ggplot(stats_demo) +
  geom_density(aes(`Demo-dailytech_Computer`), color = "blue") +
  geom_density(aes(`Demo-dailytech_Tablet`), color = "red") +
  geom_density(aes(`Demo-dailytech_SmartPhone`), color = "green") +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_dotplot(aes(`Demo-weeklygaming`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-expdataanal`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-expdatavis`)) +
  scale_x_discrete(drop=FALSE) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-expreadnetvis`)) +
  facet_grid(filename~.)

```

```{r}

ggplot(stats_demo) +
  geom_bar(aes(`Demo-expcreatenetvis`)) +
  facet_grid(filename~.)

```

```{r, eval=FALSE}

IDs <- stats_datasets_tall %>% group_by(`Demo-ResponseID`) %>% summarise(TotalDuration = sum(DatasetDuration)) %>% arrange(TotalDuration) %>% select(`Demo-ResponseID`) %>% unlist()

stats_datasets_tall$`Demo-ResponseID` <- factor(stats_datasets_tall$`Demo-ResponseID`, 
                                                levels = IDs) 

ggplot(stats_datasets_tall) +
  geom_col(aes(x = `Demo-ResponseID`, 
               y = `DatasetDuration`)) +
  facet_grid(filename~Dataset, scales="free_y", space="free_y") +
  coord_flip()

```

```{r}

ggplot(stats_datasets_tall) +
  geom_boxplot(aes(x = factor(Dataset), y = `DatasetDuration`)) +
  facet_grid(.~filename)

```

## Figures

```{r}

ggplot(graded_num_ans) +
  geom_density(aes(LogError)) + 
  scale_x_log10() +
  facet_grid(.~filename)

```


```{r}

ggplot(graded_num_ans) +
  geom_boxplot(aes(factor(Dataset), LogError)) +
  scale_y_log10() +
  facet_grid(.~filename)

```

```{r}

ggplot(graded_num_ans) +
  geom_boxplot(aes(Task, LogError)) +
  scale_y_log10() +
  facet_grid(.~filename)

```

```{r}

ggplot(graded_num_ans) +
  geom_point(aes(DatasetDuration, LogError)) +
  #scale_y_log10() + 
  facet_grid(Dataset~Task) +
  facet_grid(.~filename)

```

```{r}

ggplot(graded_num_ans) +
  geom_boxplot(aes(factor(Dataset), LogError)) +
  scale_y_log10() +
  facet_grid(Task~filename)

```

```{r}

ggplot(graded_num_ans) +
  geom_boxplot(aes(Task, LogError)) +
  scale_y_log10() +
  facet_grid(Dataset~filename)

```

```{r}

ggplot(graded_num_ans) +
  geom_boxplot(aes(Condition, LogError)) +
  facet_grid(Task~Dataset)

```

```{r clust_conf_bar}

ggplot(graded_num_ans %>% filter(!is.na(ClustConf))) +
  geom_bar(aes(x=ClustConf)) + 
  facet_grid(Dataset~.)

```

```{r}

ggplot(graded_num_ans %>% filter(!is.na(ClustConf)) %>% filter(Task == "NumClust")) +
  geom_boxplot(aes(ClustConf, LogError)) +
  scale_y_log10() +
  coord_flip() +
  facet_grid(Dataset~Condition)

```

```{r}

ggplot(graded_num_ans %>% filter(!is.na(ClustConf)) %>% filter(Task == "NumClust")) +
  geom_boxplot(aes(ClustConf, LogError)) +
  scale_y_log10() +
  #coord_flip() +
  facet_grid(.~Condition)

```

```{r}

ggplot(graded_num_ans %>% filter(!is.na(ClustConf)) %>% filter(Task == "NumClust")) +
  geom_boxplot(aes(Condition, LogError)) 


```

### Compare results to some of the demographics?


```{r, fig.width=12, fig.height=12}

ggplot() + 
  geom_point(data = node_lookup %>% filter(!(Dataset==6)),
             aes(x=NodeXAdjusted,y=NodeYAdjusted),colour="gray50") +
  geom_point(data = graded_nodes %>% filter(Task == "BC"), 
             aes(x=Click_X,y=Click_Y), colour="red") + 
  facet_grid(Dataset~Condition) +
  theme_bw()

# TO DO: Factor node_lookup$Condition

# TO DO: remove dataset 6 from raw network data, since I never used it in surveys?


```

```{r, fig.width=12, fig.height=12}

ggplot() + 
  geom_point(data = node_lookup,
             aes(x=NodeXAdjusted,y=NodeYAdjusted),colour="gray50") +
  geom_point(data = graded_nodes %>% filter(Task == "ClickHighDeg"), 
             aes(x=Click_X,y=Click_Y), colour="red") + 
  facet_grid(Dataset~Condition) +
  theme_bw()


```


```{r}

ggplot(graded_nodes %>% filter(Task == "BC")) +
  geom_point(aes(x=factor(Dataset), y=Percentage)) +
  facet_grid(filename~Condition)
  
```

```{r}

ggplot(graded_nodes %>% filter(Task == "ClickHighDeg")) +
  geom_point(aes(x=factor(Dataset), y=Percentage)) +
  facet_grid(filename~Condition)
  
```

```{r, eval=FALSE}

stats_datasets_tall %>% filter(NetVisExperience=="Low", !is.na(Dataset)) %>% group_by(`Stats-Group`,Dataset) %>% summarise(count=n_distinct(`Demo-ResponseID`)) %>% spread(Dataset,count) %>% View()

stats_datasets_tall %>% filter(NetVisExperience=="High", !is.na(Dataset)) %>% group_by(`Stats-Group`,Dataset) %>% summarise(count=n_distinct(`Demo-ResponseID`)) %>% spread(Dataset,count) %>% View()


stats_demo %>% group_by(NetVisExperience, `Stats-Group`) %>% summarise(count=n_distinct(`Demo-ResponseID`)) %>% spread(`Stats-Group`,count) %>% View()

```

## Research Questions

### Are some tasks harder?

```{r}

ggplot(graded_num_ans %>% filter(Task=="AvgDeg",Dataset!=0,Response>0)) +
  geom_violin(aes(factor(Dataset),Response, fill=factor(Dataset))) + 
  #geom_boxplot(aes(factor(Dataset),Response), width=.1, alpha=.3) + 
  geom_point(data=num_ans_lookup %>% filter(Task=="AvgDeg",!(Dataset %in% c(0,6))),aes(y=CorrectAnswer,x=factor(Dataset)))+
  scale_y_log10(limits=c(.1,10000)) +
  labs(title="Average Degree",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumNodes",Dataset!=0,Response>0)) +
  geom_violin(aes(factor(Dataset),Response, fill=factor(Dataset))) + 
  geom_point(data=num_ans_lookup %>% filter(Task=="NumNodes",!(Dataset %in% c(0,6))),aes(y=CorrectAnswer,x=factor(Dataset)))+
  scale_y_log10(limits=c(.1,10000)) +
  labs(title="Number of Nodes",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumLinks",Dataset!=0,Response>0)) +
  geom_violin(aes(factor(Dataset),Response, fill=factor(Dataset))) + 
  geom_point(data=num_ans_lookup %>% filter(Task=="NumLinks",!(Dataset %in% c(0,6))),aes(y=CorrectAnswer,x=factor(Dataset)))+
  scale_y_log10(limits=c(.1,10000)) +
  labs(title="Number of Links",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumHighDegree",Dataset!=0,Response>0)) +
  geom_violin(aes(factor(Dataset),Response, fill=factor(Dataset))) + 
  geom_point(data=num_ans_lookup %>% filter(Task=="NumHighDegree",!(Dataset %in% c(0,6))),aes(y=CorrectAnswer,x=factor(Dataset)))+
  scale_y_log10(limits=c(.1,10000)) +
  labs(title="Degree of Highest Degree Node",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumClust",Dataset!=0,Response>0)) +
  geom_violin(aes(factor(Dataset),Response, fill=factor(Dataset))) + 
  geom_point(data=num_ans_lookup %>% filter(Task=="NumClust",!(Dataset %in% c(0,6))),aes(y=CorrectAnswer,x=factor(Dataset)))+
  scale_y_log10(limits=c(.1,10000)) +
  labs(title="Number of Clusters",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="LargeClust1",Dataset!=0,Response>0)) +
  geom_violin(aes(factor(Dataset),Response, fill=factor(Dataset))) + 
  geom_point(data=num_ans_lookup %>% filter(Task=="LargeClust1",!(Dataset %in% c(0,6))),aes(y=CorrectAnswer,x=factor(Dataset)))+
  scale_y_continuous(limits=c(0,100)) +
  labs(title="% Nodes in Largest Cluster",x="Dataset") +
  guides(fill="none")

```

```{r}

ggplot(graded_num_ans %>% filter(Task=="AvgDeg",Dataset!=0)) +
  geom_violin(aes(factor(Dataset),LogError, fill=factor(Dataset))) + 
  labs(title="Average Degree",x="Dataset") +
  guides(fill="none")


ggplot(graded_num_ans %>% filter(Task=="NumNodes",Dataset!=0)) +
  geom_violin(aes(factor(Dataset),LogError, fill=factor(Dataset))) + 
  labs(title="Number of Nodes",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumLinks",Dataset!=0)) +
  geom_violin(aes(factor(Dataset),LogError, fill=factor(Dataset))) + 
  labs(title="Number of Links",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumHighDegree",Dataset!=0)) +
  geom_violin(aes(factor(Dataset),LogError, fill=factor(Dataset))) + 
  labs(title="Degree of Highest Degree Node",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="NumClust",Dataset!=0)) +
  geom_violin(aes(factor(Dataset),LogError, fill=factor(Dataset))) + 
  labs(title="Number of Clusters",x="Dataset") +
  guides(fill="none")

ggplot(graded_num_ans %>% filter(Task=="LargeClust1",Dataset!=0)) +
  geom_violin(aes(factor(Dataset),LogError, fill=factor(Dataset))) + 
  labs(title="% Nodes in Largest Cluster",x="Dataset") +
  guides(fill="none")

```

```{r}

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Phr","Col","Siz"))) +
  geom_density(aes(LogError,color=factor(Condition,levels=c("Ctrl","Phr","Col","Siz")))) +
  labs(title="Error by Graphic Condition",x="Condition") +
  scale_color_discrete(name="Condition")

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Phr","Col","Siz"))) +
  geom_boxplot(aes(y=LogError,x=factor(Condition,levels=c("Ctrl","Phr","Col","Siz")),color=factor(Condition,levels=c("Ctrl","Phr","Col","Siz")))) +
  labs(title="Error by Graphic Condition",x="Condition") +
  scale_color_discrete(name="Condition")


```

```{r}

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord"))) +
  geom_density(aes(LogError,color=factor(Condition,levels=c("Ctrl","Cir","Fru","Ord")))) +
  labs(title="Error by Layout Condition",x="Condition") +
  scale_color_discrete(name="Condition")

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord"))) +
  geom_boxplot(aes(y=LogError,x=factor(Condition,levels=c("Ctrl","Cir","Fru","Ord")),color=factor(Condition,levels=c("Ctrl","Cir","Fru","Ord")))) +
  labs(title="Error by Layout Condition",x="Condition") +
  scale_color_discrete(name="Condition")


```

```{r}

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord"))) +
  geom_density(aes(LogError,color=NetVisExperience)) +
  labs(title="Error by Expertise",x="Expertise") +
  scale_color_discrete(name="Expertise")

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord"))) +
  geom_boxplot(aes(y=LogError,x=NetVisExperience,color=NetVisExperience)) +
  labs(title="Error by Expertise",x="Expertise") +
  scale_color_discrete(name="Expertise")


```

```{r}

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord")) %>% filter(Dataset!=0)) +
  geom_violin(aes(Task,LogError, fill=Task)) + 
  labs(title="Tasks by Layout Condition") +
  guides(fill="none") +
  facet_wrap(~Condition)

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord")) %>% filter(Dataset!=0)) +
  geom_boxplot(aes(factor(Condition, levels=c("Ctrl","Cir","Fru","Ord")),LogError, fill=Condition)) + 
  labs(title="Tasks by Layout Condition",x="Condition") +
  guides(fill="none") +
  facet_wrap(~Task)

ggplot(graded_num_ans %>% filter(Condition %in% c("Ctrl","Cir","Fru","Ord")) %>% filter(Dataset!=0)) +
  geom_density(aes(LogError, color=factor(Condition, levels=c("Ctrl","Cir","Fru","Ord")))) + 
  labs(title="Tasks by Layout Condition",x="Condition") +
  scale_color_discrete(name="Condition") +
  facet_wrap(~Task)

```

```{r}

ggplot(graded_num_ans) +
  geom_density(aes(LogError))

```

### Does dataset influence accuracy?

```{r, cache=TRUE}

# https://stats.idre.ucla.edu/other/mult-pkg/whatstat/

# Dataset (IV) is ordinal, and categories are *not* independent (individuals do complete multiple datasets)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do one-way repeated measures ANOVA?

require(car)
require(foreign)

#kirk <- within(read.dta("~/Desktop/rb4.dta"), 
#    {
#        s <- as.factor(s)
#        a <- as.factor(a)
#    })

#model <- lm(y ~ a + s, data = kirk)
#analysis <- Anova(model, idata = kirk, idesign = ~s)
#print(analysis)

# Okay, here I'm assuming the syntax is lm(DV ~ IV + ParticipantIdentifier)

# LogError is not normally, so taking another log(??), 
# but there are 0 values in LogError so need to add a scalar; not sure what scalar to add, picked .1
# TO DO: adjust LogError so there are no 0s???

model <- lm(log(LogError+.1) ~ factor(Dataset) + factor(`Demo-ResponseID`), data=graded_num_ans)

# Wow!  Model is 138.3Mb

analysis <- Anova(model, idata = graded_num_ans, idesign = ~factor(`Demo-ResponseID`))

print(analysis)

# looks like Dataset is significant at p < .001???

# If LogError is ordinal or interval, can do Friedman test?

#friedman.test(formula, data, subset, na.action, ...)
#friedman.test(LogError ~ Dataset | `Demo-ResponseID`, data=graded_num_ans)

# doesn't work; error is "not an unreplicated complete block design"

```

### Does graphic design/phrasing influence accuracy?

```{r}

# Condition (IV) is nominal, and categories are independent (individuals do not complete multiple conditions)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do one-way ANOVA?

# not sure if the command below is right; if so, not significant

graphic.design <- filter(graded_num_ans, Condition %in% c("Ctrl","Phr","Col","Siz"))

summary(aov(log(LogError+.1) ~ factor(Condition),data=graphic.design))

# If LogError is ordinal or interval, can do Kruskal Wallis?

kruskal.test(LogError ~ factor(Condition), data=graphic.design)

# not significant

```

### Does layout algorithm influence accuracy?

```{r}

# Condition (IV) is nominal, and categories are independent (individuals do not complete multiple conditions)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do one-way ANOVA?

# not sure if the command below is right; if so, not significant

layout.alg <- filter(graded_num_ans, Condition %in% c("Ctrl","Cir","Fru","Ord"))

summary(aov(log(LogError+.1) ~ factor(Condition),data=layout.alg))

# If LogError is ordinal or interval, can do Kruskal Wallis?

kruskal.test(LogError ~ factor(Condition), data=layout.alg)

# p-value lower but still not significant

```

### Does expertise influence accuracy?

```{r}

# NetVisExperience (IV) is ordinal, 2 groups, and levels are independent (individuals have one level of experience)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do 2 independent sample t-test?

t.test(log(LogError+.1) ~ factor(NetVisExperience), data=graded_num_ans)

# looks significant

# If LogError is ordinal or interval, can do Wilcoxon-Mann Whitney test?

wilcox.test(LogError ~ factor(NetVisExperience), data=graded_num_ans)

# looks significant

# p-value lower but still not significant

```


### Does gender influence accuracy?

```{r}

# Demo-gender (IV) is nominal, more than 2 groups, and levels are independent (individuals have one gender)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do one-way ANOVA?

# not sure if the command below is right; if so, not significant

summary(aov(log(LogError+.1) ~ factor(`Demo-gender`),data=graded_num_ans))

# If LogError is ordinal or interval, can do Kruskal Wallis?

kruskal.test(LogError ~ factor(`Demo-gender`), data=graded_num_ans)

# not significant

```

### Does self-report previous experience with network visualization influence accuracy?

```{r}

# Demo-expreadnetvis (IV) is nominal, more than 2 groups, and levels are independent (individuals have one gender)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do one-way ANOVA?

summary(aov(log(LogError+.1) ~ factor(`Demo-expreadnetvis`),data=graded_num_ans)) # not significant
summary(aov(log(LogError+.1) ~ factor(`Demo-expcreatenetvis`),data=graded_num_ans)) # not significant

# If LogError is ordinal or interval, can do Kruskal Wallis?

kruskal.test(LogError ~ factor(`Demo-expreadnetvis`), data=graded_num_ans) # not significant
kruskal.test(LogError ~ factor(`Demo-expcreatenetvis`), data=graded_num_ans) # significant? p < .01



```

### Does dataset duration influence accuracy?

```{r}

# DatasetDuration (IV) is interval
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do correlation?  (Q: do both need to be normal?)

cor.test(~DatasetDuration+LogError,data=graded_num_ans) # barely significant (p ~ .01)

cor.test(~DatasetDuration+log(LogError+.1),data=graded_num_ans) # significant

cor.test(~log(DatasetDuration)+log(LogError+.1),data=graded_num_ans) # significant

# If LogError is interval and normal, can do simple linear regression?

lm(log(LogError+.1)~log(DatasetDuration),data=graded_num_ans) 
# don't know how well the model fits...

# If LogError is ordinal or interval, can do non-parametric correlation

cor.test(~DatasetDuration+LogError,data=graded_num_ans, method="spearman")  #significant?


```

### Do task and layout algorithm interact to influence accuracy?

```{r, cache=TRUE}

# Task (IV1) is nominal, categories are dependent (individuals complete multiple tasks)
# Condition (IV2) is nominal, categories are independent (individuals complete a single condition)
# LogError (DV) is interval, can be made normal with another log transformation???

# If LogError is interval and normal, can do mixed ANOVA?

# http://www.cookbook-r.com/Statistical_analysis/ANOVA/#mixed-design-anova

layout.alg <- filter(graded_num_ans, Condition %in% c("Ctrl","Cir","Fru","Ord")) %>% mutate(LogLog = log(LogError+.1), Condition = factor(Condition, levels=c("Ctrl","Cir","Fru","Ord")), Task = factor(Task), `Demo-ResponseID` = factor(`Demo-ResponseID`))


aov_task_condition <- aov(LogLog ~ Condition*Task + Error(`Demo-ResponseID`/Task), data=layout.alg)

# Model is quite large, 300Mb

summary(aov_task_condition) # Task is significant?  Not sure how to interpret



```

### Mixed Models



```{r}

# https://iucat.iu.edu/catalog/14518998, chapter 5

# Repeated measures, tasks and datasets (crossed)

# R by default treats the lowest category (alphabetically or numerically) of a
# categorical fixed factor as the reference category in a model

# We don't really have reference categories for the repeated measures factors, 
# though, so I guess it doesn't matter?  Maybe want to exclude training dataset

exp_only$Dataset <- factor(exp_only$Dataset)

model.lme <- lme(LogError ~ Task*Dataset, random = ~1 | DemoResponseID, method="REML", data=exp_only)

summary(model.lme)

anova(model.lme)


# can't get this to finish (see p. 220)
#model.lme.2 <- update(model.lme, random = ~ Task | DemoResponseID)

#summary(model.lme.2)

#anova(model.lme.2)

# p. 220

# We test Hypothesis 5.1 to decide if we need the random treatment effects, using a likelihood
# ratio test. This would typically not be done with such a small sample of animals
# (given the asymptotic nature of likelihood ratio tests), but we perform this test for illustrative
# purposes. The test statistic is calculated by subtracting the –2 REML log-likelihood
# value for Model 5.2 (the reference model) from that for Model 5.1 (the value of the test
# statistic is 275.3−249.2 = 26.1). The –2 REML log-likelihood values can be obtained from
# the output provided by the summary() function for each model. The test statistic has a null
# distribution that is a mixture of χ21 and χ22 distributions with equal weights of 0.5, so the
# anova() function cannot be used for the p-value. Instead, we calculate a p-value for the test
# statistic as follows:

# 0.5*(1 - pchisq(26.1,1)) + 0.5*(1 - pchisq(26.1,2))


# throws an error: "nlminb problem, convergence error code = 1 message = iteration limit reached without convergence"

#model.lme.3 <- lme(LogError ~ Task*Dataset, 
#                 random = ~ Task | DemoResponseID, 
#                 weights = varIdent(form = ~ 1 | Task),
#                 data=exp_only)

```

```{r}
#model.lme <- lme(LogError ~ Task*Dataset, random = ~1 | DemoResponseID, method="REML", data=exp_only)

model.fit.lmer <- lmer(LogError ~ Task*Dataset + (1|DemoResponseID), data = exp_only, REML = T)

summary(model.fit.lmer)

anova(model.fit.lmer)

# In general, we recommend use of the lmerTest package in R
# for users interested in testing hypotheses about parameters estimated using the lmer()
# function. In this chapter, we illustrate likelihood ratio tests using selected functions
# available in the lme4 package. See Chapter 3 for an example of using the lmerTest
# package, in the case of a random intercept model.

ranef(model.fit.lmer)

```

```{r}
model.fit.lmer.2 <- lmer(LogError ~ Task*Dataset + (Task|DemoResponseID), data = exp_only, REML = T)

summary(model.fit.lmer.2)


```

```{r eval=FALSE}

# Clustered Longitudinal Data (Chapter 7)

#model7.1.fit <- lme(gcf ~ time + 
#                   base_gcf + cda + 
#                   age + 
#                   time:base_gcf + time:cda + 
#                   time:age, 
#                   random = list(patient = ~time, tooth = ~1), 
#                   data = veneer, 
#                   method = "REML")

clust.model.fit <- lme(LogError ~ Task + TaskOrder +
                         Dataset + DatasetOrder +
                         Task:TaskOrder + 
                         Task:Dataset + Task:DatasetOrder,
                       random = list(Condition = ~Task, `DemoResponseID` = ~1),
                       data = exp_only,
                       method = "REML")

summary(clust.model.fit)

intervals(clust.model.fit)

random.effects(clust.model.fit)

```


## Output


```{r, eval=FALSE}

# add charts to Word Doc

# ggplot example
gg <- ggplot(mtcars, aes(x = mpg , y = wt, colour = qsec)) + geom_point() + theme_minimal()

# produce an emf file containing the ggplot (won't be editable in Word, but will be vector)
filename <- tempfile(fileext = ".emf")
emf(file = filename, width = 6, height = 7)
print(gg)
dev.off()

read_docx() %>% 
  body_add_img(src = filename, width = 6, height = 7) %>% 
  print(target = "~/Desktop/demo_emf.docx") %>% 
  invisible()

# add charts to PowerPoint (should be editable, though they won't be a chart per se)

read_pptx() %>% 
  add_slide(layout = "Title and Content", master = "Office Theme") %>% 
  ph_with_vg(code = print(gg), type = "body") %>% 
  print(target = "~/Desktop/demo_rvg.pptx") %>% 
  invisible()


```

