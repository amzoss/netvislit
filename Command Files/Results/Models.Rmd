---
title: "Models for Combined NetVisLit Data"
author: "Angela Zoss"
date: "January 17, 2018"
output: github_document
---

Notes:

These command files should contain commands that open up your analysis data files, and then use those data to generate the output upon which your results are based.

Every command that generates any of your results should be preceded by a comment that states which result the command generates.  A few Hypothetical examples illustrate what these comments might look like:

* The following command generates the first column of Table 6.

The command files for your analysis phase should not contain any commands that generate new variables or process your data in any way.  All the procedures required to prepare your data for analysis should be executed by the command files you wrote for the processing phase.

It is often convenient to write all the commands for the analysis phase in a single command file. However, if the nature of your project or the structure of your data are such that you think it would make sense to divide the code that generates the results into two or more command files, you should feel free to do so.  No matter how you organize your analysis command files, your Read Me file will include an explanation of how to use them to reproduce your results.

Save the command files you write for the analysis phase in the Command Files folder.

```{r setup}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```


## Load packages

```{r, message=FALSE}

require(tidyverse)

# mixed modeling

library(nlme)
library(lme4)
#library(car)
library(lmerTest)

# negative binomial

require(foreign)
require(MASS)

# logistic

library(pscl)
library(aod)

# output

#library(officer)
#library(rvg)
#library(devEMF)

```

## Environmental Variables

```{r}

originalDataDir <- "../../Original Data"
analysisDataDir <- "../../Analysis Data"

generatedDataDir <- file.path(originalDataDir, "Generated data")

figureDir <- "../../Documents/"

```

## Loading analysis data files

```{r, message=FALSE}

graphics <- read_csv(file.path(analysisDataDir, "Graphics.csv"))

layoutfac <- read_csv(file.path(analysisDataDir, "LayoutFac.csv"))

```

## Slight processing for analysis

```{r}

graphics$ClustConf <- factor(graphics$ClustConf,
                             levels = c("Very confident (76-100%)", 
                                        "Somewhat confident (51-75%)", 
                                        "Somewhat doubtful (26-50%)", 
                                        "Very doubtful (0-25%)"),
                             ordered = TRUE)



```


## Mixed Models

TO DO: try all covariates again, separate model for each task

### Graphics subset

```{r}

graphics_lme <- graphics

graphics_lme$Dataset <- factor(graphics_lme$Dataset, ordered = TRUE)

graphics_lme <- set_tidy_names(graphics_lme, syntactic=TRUE)

#Make control the reference group
#TO DO: decide on reference group for task??

graphics_lme <- graphics_lme %>% mutate(Condition1 = factor(
  case_when(
  Condition == "Ctrl" ~ 0,
  Condition == "Phr" ~ 1,
  Condition == "Col" ~ 2,
  Condition == "Siz" ~ 3)
  )
)

graphics_lme$Condition <- factor(graphics_lme$Condition)

graphics_lme$Condition <- relevel(graphics_lme$Condition, ref = "Ctrl")


```

```{r}

# separate datasets for each Task

graphics_avgdeg <- graphics_lme %>% filter(Task == "AvgDeg")

graphics_bc <- graphics_lme %>% filter(Task == "BC")

graphics_clickhighdeg <- graphics_lme %>% filter(Task == "ClickHighDeg")

graphics_lgclust <- graphics_lme %>% filter(Task == "LargeClust1")

graphics_numclust <- graphics_lme %>% filter(Task == "NumClust")

graphics_numhighdeg <- graphics_lme %>% filter(Task == "NumHighDegree")

graphics_numlinks <- graphics_lme %>% filter(Task == "NumLinks")

graphics_numnodes <- graphics_lme %>% filter(Task == "NumNodes")


```

### Average Degree

```{r, cache=TRUE}

# https://iucat.iu.edu/catalog/14518998, chapters 3 and 5
# data at http://www-personal.umich.edu/~bwest/almmussp.html

# testing models, no nesting/grouping

# Repeated measures, tasks and datasets (crossed)

# starting with older package (nlme), with function lme()

# R by default treats the lowest category (alphabetically or numerically) of a
# categorical fixed factor as the reference category in a model

# We don't really have reference categories for the repeated measures factors, 
# though, so I guess it doesn't matter?  Maybe want to exclude training dataset

# Note about syntax; can use either ":" or "*" for interaction, but using "*" automatically adds
# the main effects of each factor into the model, too, instead of just the interaction

graph.avgdeg.lme <- lme(LogError ~ Dataset, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)


# Dataset is not significant for AvgDeg; try Condition

graph.avgdeg.lme <- lme(LogError ~ Condition, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Condition is not significant; try TaskOrder

graph.avgdeg.lme <- lme(LogError ~ TaskOrder, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# TaskOrder is not significant; try DatasetOrder

graph.avgdeg.lme <- lme(LogError ~ DatasetOrder, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# DatasetOrder is not significant; try DatasetDuration

graph.avgdeg.lme <- lme(LogError ~ DatasetDuration, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(DatasetDuration))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# DatasetDuration is not significant; try Stats.Q_TotalDuration

graph.avgdeg.lme <- lme(LogError ~ Stats.Q_TotalDuration, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Stats.Q_TotalDuration is not significant; try Stats.OperatingSystem

graph.avgdeg.lme <- lme(LogError ~ Stats.OperatingSystem, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Stats.OperatingSystem is not significant; try StatsNumPixels

graph.avgdeg.lme <- lme(LogError ~ StatsNumPixels, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# StatsNumPixels is not significant; try Demo.age

graph.avgdeg.lme <- lme(LogError ~ Demo.age, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.age))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.age is not significant; try Demo.gender

graph.avgdeg.lme <- lme(LogError ~ Demo.gender, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.gender))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.gender is not significant; try Demo.lang

graph.avgdeg.lme <- lme(LogError ~ Demo.lang, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.lang))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# TO DO: try processing Demo.lang more?  English vs. not English?

# Demo.lang is not significant; try Demo.educ

graph.avgdeg.lme <- lme(LogError ~ Demo.educ, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.educ))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# TO DO: try processing Demo.educ more?  at least factor it?

# Demo.educ is not significant; try Demo.acfield

graph.avgdeg.lme <- lme(LogError ~ Demo.acfield, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.acfield))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.acfield is not significant; try Demo.dailytech_Computer

graph.avgdeg.lme <- lme(LogError ~ Demo.dailytech_Computer, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_Computer))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.dailytech_Computer is not significant; try Demo.dailytech_Tablet

graph.avgdeg.lme <- lme(LogError ~ Demo.dailytech_Tablet, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_Tablet))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.dailytech_Tablet is not significant; try Demo.dailytech_SmartPhone

graph.avgdeg.lme.SP <- lme(LogError ~ Demo.dailytech_SmartPhone, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))))

# TO DO : figure out if this should be a factor

summary(graph.avgdeg.lme.SP)

anova(graph.avgdeg.lme.SP)

## Demo.dailytech_SmartPhone *is* significant

ggplot(graphics_avgdeg) +
  geom_point(aes(Demo.dailytech_SmartPhone, LogError)) +
  geom_smooth(aes(Demo.dailytech_SmartPhone, LogError), method="lm")

# try Demo.weeklygaming

graph.avgdeg.lme <- lme(LogError ~ Demo.weeklygaming, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.weeklygaming))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.weeklygaming not significant; try Demo.expdataanal
# TO DO: need to factor, set levels

graph.avgdeg.lme <- lme(LogError ~ Demo.expdataanal, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.expdataanal))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.expdataanal not significant; try Demo.expdatavis
# TO DO: need to factor, set levels

graph.avgdeg.lme <- lme(LogError ~ Demo.expdatavis, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.expdatavis))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.expdatavis not significant; try Demo.expreadnetvis
# TO DO: need to factor, set levels

graph.avgdeg.lme.RNV <- lme(LogError ~ Demo.expreadnetvis, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.expreadnetvis))))

summary(graph.avgdeg.lme.RNV)

anova(graph.avgdeg.lme.RNV)

# Demo.expreadnetvis barely significant (p=0.0494); try just the experts (only 1, actually)
# TO DO: need to factor, set levels

graphics_avgdeg <- graphics_avgdeg %>% mutate(Demo.expreadnetvis.alot = ifelse(Demo.expreadnetvis == "A lot",1,0))

graph.avgdeg.lme.RNVAL <- lme(LogError ~ Demo.expreadnetvis.alot, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.expreadnetvis.alot))))

summary(graph.avgdeg.lme.RNVAL)

anova(graph.avgdeg.lme.RNVAL)

# better, but still barely significant; p = 0.0106

# try Demo.expcreatenetvis
# TO DO: need to factor, set levels

graph.avgdeg.lme <- lme(LogError ~ Demo.expcreatenetvis, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.expcreatenetvis))))

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Demo.expcreatenetvis not significant; try AvgDeg

graph.avgdeg.lme <- lme(LogError ~ AvgDeg, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# AvgDeg not significant; try Density

graph.avgdeg.lme <- lme(LogError ~ Density, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Density not significant; try LargeClust1

graph.avgdeg.lme <- lme(LogError ~ LargeClust1, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# LargeClust1 not significant; try Modularity

graph.avgdeg.lme <- lme(LogError ~ Modularity, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Modularity not significant; try NumClust

graph.avgdeg.lme <- lme(LogError ~ NumClust, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# NumClust not significant; try NumHighDegree

graph.avgdeg.lme <- lme(LogError ~ NumHighDegree, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# NumHighDegree not significant; try NumLinks

graph.avgdeg.lme <- lme(LogError ~ NumLinks, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# NumLinks not significant; try NumNodes

graph.avgdeg.lme <- lme(LogError ~ NumNodes, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# NumNodes not significant; try NumNodesClust1

graph.avgdeg.lme <- lme(LogError ~ NumNodesClust1, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg)

summary(graph.avgdeg.lme)

anova(graph.avgdeg.lme)

# Final; try both significant factors

graph.avgdeg.lme.SP.RNVAL <- lme(LogError ~ Demo.dailytech_SmartPhone + Demo.expreadnetvis.alot, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))) %>% filter(!(is.na(Demo.expreadnetvis.alot))))

summary(graph.avgdeg.lme.SP.RNVAL)

anova(graph.avgdeg.lme.SP.RNVAL)

# Better than just one predictor?

anova(graph.avgdeg.lme.SP.RNVAL, graph.avgdeg.lme.SP)

# no significant difference

graph.avgdeg.lme.RNVAL <- lme(LogError ~ Demo.expreadnetvis.alot, random = ~ 1 | Demo.ResponseID, method="REML", data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))) %>% filter(!(is.na(Demo.expreadnetvis.alot))))

anova(graph.avgdeg.lme.SP.RNVAL, graph.avgdeg.lme.RNVAL)

# no significant difference; guess we can use both predictors? or maybe should just use stronger
# predictor, SmartPhone?


random.effects(graph.avgdeg.lme.SP)

# do we really need random effects?  test model without random effects using gls

graph.avgdeg.gls <- gls(LogError ~ Demo.dailytech_SmartPhone, data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))))

anova(graph.avgdeg.lme.SP, graph.avgdeg.gls)

# result of anova is significant (p=0.0019), indicating that there is a significant difference when
# you remove random effects, so should leave them in

```

```{r, eval=FALSE}

# NOTE: nesting is not appropriate, but syntax for tests might be useful in future

# model with nesting; takes *forever* but does finish
# can nest when I recode Condition as 0, 1, 2, 3 (Condition1), 
# but if I just list Ctrl as the ref (Condition), won't converge

model.lme.update.nest <- update(model.lme, random = ~ Condition1 | Demo.ResponseID)

summary(model.lme.update.nest)

anova(model.lme.update.nest)

# might want to check whether nesting makes a difference, but can't run anova because 
# "the test statistic has a null distribution that is a mixture of X12 and X22 distributions
# with equal weights of 0.5, so the anova() function cannot be used for the p-value." (p.220)

# Check summary of each model for -2 REML log-likelihood values 

# "logLik" value for unnested: -14415.3
# "logLik" value for nested: -14441
# -2 REML logLik is just -2 * the logLik value

# test statistics = unnested - nested
test.stat <- abs((-2*-14415.3) - (-2*-14441)) # ~51.4

# note: when test.stat was negative, p-value was not significant, but highly significant when
# positive
p.val <- 0.5*(1-pchisq(test.stat,1)) + 0.5*(1-pchisq(test.stat,2))

# p-value is < 0.001, so nesting is significantly different


```

```{r, cache=TRUE}

# adding weights to have a separate residual variance for each treatment group
# TO DO: are these the only weights I need to consider???  check that these weights make sense

graph.avgdeg.weight <- lme(LogError ~ Demo.dailytech_SmartPhone, 
                 random = ~ 1 | Demo.ResponseID, 
                 method="REML", 
                 data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))),
                 weights = varIdent(form = ~1 | Condition))

summary(graph.avgdeg.weight)

# Parameter estimates are:
#    Ctrl      Siz      Phr      Col 
#1.000000 1.486356 1.409153 1.436532 

anova(graph.avgdeg.lme.SP, graph.avgdeg.weight)

# result is significant (p<.0001), which suggests that we should retain the second model with the 
# weights,  otherwise known as the "heterogeneous variances model" (p. 94)


```

```{r, cache=TRUE}


graphics_avgdeg <- graphics_avgdeg %>% mutate(Ctrl_dummy = 
                                                case_when(
                                                  Condition == "Ctrl" ~ 1,
                                                  Condition != "Ctrl" ~ 2)
                                              )

#graphics_lme$Ctrl_dummy[graphics_lme$Condition == "Ctrl"] <- 1
#graphics_lme$Ctrl_dummy[graphics_lme$Condition != "Ctrl"] <- 2

graph.avgdeg.weight.pooled <- lme(LogError ~ Demo.dailytech_SmartPhone, 
                 random = ~ 1 | Demo.ResponseID, 
                 method="REML", 
                 data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))),
                 weights = varIdent(form = ~1 | Ctrl_dummy))

anova(graph.avgdeg.weight, graph.avgdeg.weight.pooled)

# test is not significant, so should not keep the weights pooled? p. 95 not clear, since in 
# example difference is not significant but pooled model is retained anyway

summary(graph.avgdeg.weight)
anova(graph.avgdeg.weight)

```

```{r, cache=TRUE}

# TO DO: need to conduct Type I F-test (section 5.7)? to try to remove nonsignficant fixed effects?
# note p. 223 explains that large F-statistics can indicate significance, even when you don't compute
# p-values

# another way to test if each additional item is significant:
# create two models, one with and one without the fixed effect
# use ML estimation (REML = F for lmer()) for each
# use anova() to compare the models (see 4.4.3.2)

# On p. 238, talks about needing to be careful about interpretation because an interaction is
# also significant; can investigate "estimated marginal means" of the combinations of levels in the
# interaction (needs to be pairwise for the interaction?)

# On p. 133, "Post-hoc comparisons can also be computed in R with 
# some additional programming (Faraway, 2005)"

# From Faraway, 2015, p. 20:
# https://search.library.duke.edu/search?id=DUKE008022150

# "We can extract the regression quantities we need from the model object. Commonly used are residuals(),
# fitted(), df.residual() which gives the degrees of freedon, deviance() which gives the RSS and
# coef() which gives the ^Beta."

modsum <- summary(graph.avgdeg.weight)
names(modsum)
modsum$logLik * -2

modsum$sigma # 0.711... what is sigma, exactly?

```


```{r, cache=TRUE}

# Residuals plot
plot(graph.avgdeg.weight)

# Residuals vs. SmartPhone, scatterplot
plot(graph.avgdeg.weight, Demo.dailytech_SmartPhone ~ resid(.))

# Predicted vs. SmartPhone
plot(graph.avgdeg.weight, Demo.dailytech_SmartPhone ~ fitted(.))


# Predicted vs. Actual LogError
plot(graph.avgdeg.weight, LogError ~ fitted(.), abline = c(0,1))

qqnorm(graph.avgdeg.weight, abline = c(0,1))

qqnorm(graph.avgdeg.weight, ~ resid(., type = "p") | Demo.dailytech_SmartPhone, abline = c(0,1))

qqnorm(graph.avgdeg.weight, ~ resid(., type = "p") | Condition, abline = c(0,1))

qqnorm(graph.avgdeg.weight, ~ranef(.))


```

### Graphics subset, linear mixed model (lme4)

```{r, cache=TRUE}

# Trying dataset first

graph.avgdeg.lmer <- lmer(LogError ~ Dataset + (1|Demo.ResponseID), data = graphics_avgdeg, REML = T)

lmsum <- summary(graph.avgdeg.lmer)
lmsum
#names(lmsum)

anova(graph.avgdeg.lmer)

# Dataset not significant; trying SmortPhone

graph.avgdeg.lmer.SP <- lmer(LogError ~ Demo.dailytech_SmartPhone + (1|Demo.ResponseID), data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))), REML = T)

summary(graph.avgdeg.lmer.SP)

anova(graph.avgdeg.lmer.SP)

# Smartphone is significant; trying readnetvisalot

graph.avgdeg.lmer.RNVAL <- lmer(LogError ~ Demo.expreadnetvis.alot + (1|Demo.ResponseID), data=graphics_avgdeg %>% filter(!(is.na(Demo.expreadnetvis.alot))), REML = T)

summary(graph.avgdeg.lmer.RNVAL)

anova(graph.avgdeg.lmer.RNVAL)

# slightly significant; trying both

graph.avgdeg.lmer.SP.RNVAL <- lmer(LogError ~ Demo.dailytech_SmartPhone + Demo.expreadnetvis.alot + (1|Demo.ResponseID), data=graphics_avgdeg %>% filter(!(is.na(Demo.dailytech_SmartPhone))) %>% filter(!(is.na(Demo.expreadnetvis.alot))), REML = T)

summary(graph.avgdeg.lmer.SP.RNVAL)

anova(graph.avgdeg.lmer.SP.RNVAL)

```

```{r, eval=FALSE}

# Try adding condition as a nesting variable for random participant effects; ultimately not appropriate to do so

#model.fit.lmer.nest <- lmer(LogError ~ Dataset + Task + Task:Dataset + (Condition1|Demo.ResponseID), data = graphics_lme, REML = T)

# error: 
# unable to evaluate scaled gradientModel failed to converge: degenerate  Hessian with 1 negative eigenvalues 

model.fit.lmer.nest <- lmer(LogError ~ Dataset + Task + Task:Dataset + (Condition|Demo.ResponseID), data = graphics_lme, REML = T)

# warning 
# Model is nearly unidentifiable: large eigenvalue ratio - Rescale variables?

nestsum <- summary(model.fit.lmer.nest)
nestsum

# want to compare unnested and nested; not supposed to do anova, so should
# create test statistic by subtracting -2 REML log-likelihood values from
# each model

# "logLik" value for unnested: -14441.77
# "logLik" value for nested: -14440.5
# -2 REML logLik is just -2 * the logLik value

# test statistics = unnested - nested
test.stat <- (-2*as.numeric(lmsum$logLik)) - (-2*as.numeric(nestsum$logLik)) # ~2.53


p.val <- 0.5*(1-pchisq(test.stat,1)) + 0.5*(1-pchisq(test.stat,2))

# p-value ~ 0.1969, so nesting is not significantly different

```



```{r, cache=TRUE}

rand(graph.avgdeg.lmer.SP)

# result shows that random effects of participant are significant (p=0.002)

anova(graph.avgdeg.lmer.SP)

# SmartPhone significant, < 0.01

#ranef(graph.avgdeg.lmer.SP)

# displays the random effects; not that useful

# unlike lme(), lmer() doesn't allow for heterogeneous error variance structures (the "weights")


```

```{r, eval=FALSE}

# From Faraway (2015), p. 156 - use step() to start with a complex model and 
# systematically remove each effects

graph.avgdeg.full.lmer <- lmer(LogError ~ Dataset + Condition + 
                           QuestionOrder + (1|Demo.ResponseID), 
                         data = graphics_avgdeg, REML = T)

step(graph.avgdeg.full.lmer)

# not sure this is useful

```

```{r, eval=FALSE}

# Clustered Longitudinal Data (Chapter 7)
# Data are not clustered, so can skip

#model7.1.fit <- lme(gcf ~ time + 
#                   base_gcf + cda + 
#                   age + 
#                   time:base_gcf + time:cda + 
#                   time:age, 
#                   random = list(patient = ~time, tooth = ~1), 
#                   data = veneer, 
#                   method = "REML")

clust.model.fit <- lme(LogError ~ Task + TaskOrder +
                         Dataset + DatasetOrder +
                         Task:TaskOrder + 
                         Task:Dataset + Task:DatasetOrder,
                       random = list(`Demo.ResponseID` = ~Condition),
                       data = graphics_lme,
                       method = "REML")

summary(clust.model.fit)

intervals(clust.model.fit, which="fixed")

random.effects(clust.model.fit)

```



```{r}

plot(graph.avgdeg.lmer.SP)

plot(graph.avgdeg.lmer.SP, resid(., scaled=TRUE) ~ fitted(.), abline = 0)

plot(graph.avgdeg.lmer.SP, resid(.) ~ fitted(.) | Task, abline = 0)

plot(graph.avgdeg.lmer.SP, resid(., scaled=TRUE) ~ fitted(.) | Task, abline = 0)

plot(graph.avgdeg.lmer.SP, LogError ~ fitted(.), abline = c(0,1))



```

```{r}

graph.avgdeg.lmer.SP.f <- fortify(graph.avgdeg.lmer.SP)

ggplot(graph.avgdeg.lmer.SP.f, aes(.fitted,.resid)) + 
  geom_point() +
  #facet_grid(.~Sex) + 
  geom_hline(yintercept=0)

ggplot(graph.avgdeg.lmer.SP.f, aes(.fitted,LogError)) + 
  geom_point() +
  geom_abline(aes(slope = 1, intercept = 0))


```

```{r}

# TO DO: check out interpretation for these plots??

library(lattice)

prof <-  profile(graph.avgdeg.lmer.SP, optimizer="Nelder_Mead", which="beta_")

prof.CI <- confint(prof)

#CI2 <- confint(graph.avgdeg.lmer.SP, maxpts = 8)

xyplot(prof)

xyplot(prof, absVal = TRUE)

xyplot(prof, conf = c(0.95, 0.99), main = "95% and 99% profile() intervals")

# can also apply logProf() and varianceProf() to profile object

densityplot(prof)

splom(prof)

```

```{r, eval=FALSE}

# doesn't make sense for continuous predictor???

lsm.task <- lsmeansLT(model.fit.lmer, test.effs = "Task")

plot(lsm.task)

lsm.task.df <- as_data_frame(lsm.task$lsmeans.table)

lsm.task.df

lsm.task.df$Task <- factor(lsm.task.df$Task, levels=lsm.task.df %>% arrange(desc(Estimate)) %>% select(Task) %>% unlist())

lsm.task.df %>% arrange(desc(Estimate))


ggplot(lsm.task.df) +
  geom_point(aes(x=Task,y=Estimate, color=`p-value`<.01)) +
  geom_errorbar(aes(x=Task,ymax=`Upper CI`,ymin=`Lower CI`), width=.2) +
  coord_flip()

# TO DO: add a color scale so TRUE/FALSE values are always same color across all plots


```



```{r, eval=FALSE}

# doesn't make sense for continuous predictor???

difflsm.task <- difflsmeans(model.fit.lmer, test.effs = "Task")

plot(difflsm.task)

difflsm.task.df <- as_data_frame(difflsm.task$diffs.lsmeans.table)

difflsm.task.df

difflsm.task.df <- difflsm.task.df %>% mutate(Pair=rownames(.)) %>% separate(Pair, c("del","Pair"), sep=5) %>% select(-del) %>% separate(Pair, c("From", "del", "To"), sep="[ ]", remove=FALSE) %>% select(-del)

difflsm.task.df$Pair <- factor(difflsm.task.df$Pair, levels=difflsm.task.df %>% arrange(desc(Estimate)) %>% select(Pair) %>% unlist())

difflsm.task.df %>% arrange(desc(Estimate))

ggplot(difflsm.task.df) +
  geom_point(aes(x=Pair,y=Estimate, color=`p-value`<.01)) +
  geom_errorbar(aes(x=Pair,ymax=`Upper CI`,ymin=`Lower CI`), width=.5) +
  geom_hline(aes(yintercept=0)) +
  coord_flip()

ggplot(difflsm.task.df) +
  geom_tile(aes(x=To,y=From,fill=Estimate)) +
    scale_fill_distiller(type="div", palette=4)

ggplot(difflsm.task.df) +
  geom_count(aes(x=To,y=From,size=abs(Estimate),fill=Estimate, color=`p-value`<.01), shape=21) +
  scale_fill_distiller(type="div", palette=4) +
  scale_color_manual(values=c("grey90","black"))
  



```



#### Layout subset, linear mixed model (nlme)
#### Layout subset, linear mixed model (lme4)

### Logistic regression for over/underestimation

```{r, eval=FALSE}

# https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/

# https://stats.idre.ucla.edu/r/dae/logit-regression/

graphics.est <- graphics_lme %>% mutate(underest = ifelse(RawDifference<0,1,0)) 

graphics.est$underest <- factor(graphics.est$underest, levels=c(1,0), labels = c("underestimated","overestimated"))

graphics.est$Task <- factor(graphics.est$Task)

xtabs(~underest + Task, data = graphics.est)

# difference between family="binomial" and family=binomial(link='logit') ?

#log.model <- glm(underest ~ Task, family=binomial(link='logit'), data=graphics.est)

log.model <- glm(underest ~ Task, family="binomial", data=graphics.est)

summary(log.model)
# Interpretation of Estimate: compared to reference task (AvgDegree), each task changes
# the log odds of underestimation by the estimate shown; negative estimates reduce
# log odds of underestimation compared to AvgDegree

## CIs using profiled log-likelihood
confint(log.model)

## CIs using standard errors
confint.default(log.model)

## Wald test for effect of Task, which is terms 2:8 in the model
wald.test(b = coef(log.model), Sigma = vcov(log.model), Terms = 2:8)

## test two specific tasks against each other (NumHighDegree and NumNodes)
l <- cbind(0,0,0,0,0,1,0,-1)
wald.test(b = coef(log.model), Sigma = vcov(log.model), L = l)
# p = 0.015, so only marginally significant

## odds ratios only
exp(coef(log.model))

## odds ratios and 95% CI
exp(cbind(OR = coef(log.model), confint(log.model)))

# https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/ 

## testing fit

# likelihood ratio test
with(log.model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
# p-value is 0, so model with predictor definitely fits better than the null model

# testing the quality of the variable, is this the best model we can get?

# is this a good fitting model?  check % accuracy; split data into training test, check accuracy on test

anova(log.model, test="Chisq")

pR2(log.model)

# don't know how to interpret; should be looking at McFadden as a sort of pseudo R^2?

# https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/
# "If comparing two models on the same data, McFaddenâ€™s would be higher for the model with the greater likelihood."

```

### Beta regression for percentage



### Negative binomial model for responses

```{r, eval=FALSE}

# https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/

# More details can be found in the Modern Applied Statistics with S by W.N. Venables and B.D. Ripley 
# (the book companion of the MASS package).

# For additional information on the various metrics in which the results can be presented, and the 
# interpretation of such, please see Regression Models for Categorical Dependent Variables Using Stata, 
# Second Edition by J. Scott Long and Jeremy Freese (2006).

nbin.graph <- glm.nb(Response ~ Task, data=graphics_lme) 

summary(nbin.graph)

nbin.graph.2 <- update(nbin.graph, . ~ . + Dataset)

summary(nbin.graph.2)

anova(nbin.graph, nbin.graph.2)

# negative binomial models assume the conditional means are not equal to the conditional variances
# This inequality is captured by estimating a dispersion parameter (not shown in the output) that is held constant in a Poisson model.

# compare negative binomial to poisson

pois.graph <- glm(Response ~ Task + factor(Dataset), family = "poisson", data = graphics_lme)

pchisq(2 * (logLik(nbin.graph.2) - logLik(pois.graph)), df = 1, lower.tail = FALSE)

# result is 
# 'log Lik.' 0 (df=14)
# not sure if that means assumptions for negative binomial are met, but proceeding with nbin
```


```{r, eval=FALSE}

(est <- cbind(Estimate = coef(nbin.graph.2), confint(nbin.graph.2)))

exp(est)

```

```{r, eval=FALSE}

nbin.graph.full <- glm.nb(Response ~ Task + factor(Dataset) + Condition + Task:Dataset + Task:Condition + Dataset:Condition, data = graphics_lme)

summary(nbin.graph.full)

nbin.graph.3 <- update(nbin.graph.full, . ~ . - Condition:Dataset)

summary(nbin.graph.3)

anova(nbin.graph.full, nbin.graph.3)

```
