---
title: "Processing Combined NetVisLit Data"
author: "Angela Zoss"
date: "June 23, 2017"
output: github_document
---

Possible data processing steps:

* Having your software open your importable data files.
* Cleaning the data to resolve any errors or discrepancies.
* Removing variables or cases that you do not need. 
* Combining data from different importable data files.
* Transposing a data table so that columns become rows and rows become columns.
* Generating new variables
* Saving intermediate and analysis data files.

## Detach previous packages

```{r}

detachAllPackages <- function() {

  basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")

  package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]

  package.list <- setdiff(package.list,basic.packages)

  if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)

}

detachAllPackages()

```

## Load packages

```{r}

require(tidyverse)
require(readxl)

```


## Environmental Variables

```{r}

originalDataDir <- "../../Original Data"
analysisDataDir <- "../../Analysis Data"

generatedDataDir <- file.path(originalDataDir, "Generated data")
path_to_qualtrics <- file.path(originalDataDir,"Raw data/Qualtrics data")
path_to_responses <- file.path(path_to_qualtrics,"responses")

```

## Loading raw data files

```{r, echo=FALSE}

# Make sure none of the files are open, or Excel will have a temp file in the same directory that messes this up

# names and rawdata files need the same basename to be matched up together

new_names <- dir(path = path_to_responses, pattern = "*.xlsx", full.names = TRUE) %>% 
  set_names(., sub("-new-names.xlsx","",basename(.))) %>%
  map(read_excel) %>% lapply(function(x){select(x, NewName)})
  #map_df(read_excel, .id="filename")

all_responses <- dir(path = path_to_responses, pattern = "*.csv", full.names = TRUE) %>% 
  set_names(., sub("-rawdata.csv","",basename(.))) %>%
  map(read_csv, skip=2, col_names=FALSE, col_types = cols(.default = "c"))

# hate using for loop, but can't figure out any other way to automate

for (file in names(all_responses)){
  nm <- unlist(new_names[[file]])
  all_responses[[file]] <- setNames(all_responses[[file]],nm)
}

raw_responses_full <- type_convert(bind_rows(all_responses, .id="filename"))


num_ans_lookup <- read_csv(file.path(generatedDataDir, "numerical_answer_lookup.csv"))

node_lookup <- read_csv(file.path(generatedDataDir, "node_lookup.csv"), col_types = cols(MaxValue = col_double(),NodeValue = col_double()))

all_nodes <- read_csv(file.path(generatedDataDir, "all_nodes.csv"))


block_ids <- read_tsv(file.path(path_to_qualtrics, "blockIDs.csv"))

population_experience <- read_csv(file.path(path_to_qualtrics,"population_experience.csv"))

training_task_order <- read_csv(file.path(path_to_qualtrics, "training_task_order.csv"))


```

## Process raw data

```{r}

# Remove bizarre R0 version of the ClickHighDeg question; 
# I must have defined a region of interest, but have no need for it

raw_responses <- raw_responses_full %>% select(-contains("R0"),-contains("R1"))

# remove responses where the "Finished" flag is false
# (for now, not doing this.  For faculty, may want to data that is technically unfinished.)

#raw_responses <- raw_responses %>% filter(`Extra-Finished`==1)

```


## Subset the experimental and training columns

```{r}

exp_responses <- dplyr::select(raw_responses, "Demo-ResponseID", starts_with("Exp"), starts_with("Training"))

exp_responses_tall <- gather(exp_responses, "Question", "Response", -1)

exp_responses_split <- separate(exp_responses_tall,Question,c("QType","Condition","Dataset","Task"),sep="[-]", convert=TRUE)

# remove unfortunate underscore in one of the tasks

exp_responses_split$Task <- sub("LargeClust_1","LargeClust1",exp_responses_split$Task)

#also separating click questions from coords, so the join with QuestionOrder data works better

exp_responses_split <- separate(exp_responses_split,Task,c("Task","Coord"),sep="[_]", convert=TRUE)

#exp_responses_split$Dataset <- as.numeric(exp_responses_split$Dataset)

# change skipped responses to NA

exp_responses_split$Response <- sub("-99",NA,exp_responses_split$Response)

# remove all NA responses

exp_responses_split <- exp_responses_split[!is.na(exp_responses_split$Response),]

# Save file out?

#write_csv(exp_responses_split, file.path(analysisDataDir,"ExperimentalResponses.csv"))

```

## Process the demographic/stats columns

```{r}

# Join demo data to exp response data?  What files need this data?  All graded files?

demo <- dplyr::select(raw_responses, "Demo-ResponseID", starts_with("Demo"))

stats <- dplyr::select(raw_responses, "filename", "Demo-ResponseID", starts_with("Stats"), -contains("training"), -contains("dataset-"))

stats$`Stats-CompensationCondition` <- sub(" ","",stats$`Stats-CompensationCondition`)

# When joining demo/stats to graded answers, can join stats_demo as is, but want to make sure stats_datasets_tall joins by both ID and Dataset

stats_demo <- full_join(stats,demo)

stats_demo <- left_join(stats_demo, population_experience)

# Replace all -99s with NA 
# (important for numerical columns, but maybe for character columns should do "Skipped")

stats_demo <- stats_demo %>% mutate_if(is.integer,(funs(ifelse(.==-99,NA,.))))
stats_demo <- stats_demo %>% mutate_if(is.character,(funs(ifelse(.=="-99","Skipped",.))))




```


## Process the order columns

```{r}

# Extract dataset order questions to new table, including RespondentID!

dataset_order <- dplyr::select(raw_responses, "Demo-ResponseID", starts_with("OrderDatasets"), -`OrderDatasetsWithinBlock-Training`)

# Transpose 5 order questions from wide to tall into two columns - column labels and values

dataset_order <- dataset_order %>% gather("Condition","Order",-1)

# Remove NAs?

dataset_order <- dataset_order %>% filter(!is.na(Order))


# Split column labels to get block in its own column

dataset_order$Condition <- sub("OrderDatasetsWithinBlock-","",dataset_order$Condition)

# Split Values to get the ordered datasets in separate columns

dataset_order <- dataset_order %>% separate(Order, c("1","2","3"), sep="[|]")

#dataset_order <- dataset_order %>% separate(Order, c("1"), sep="[|]")

# Transpose dataset columns into one column for dataset number and another for dataset order

dataset_order <- dataset_order %>% gather("DatasetOrder","BlockID",-c(1:2), convert=TRUE)

dataset_order <- dataset_order %>% filter(!is.na(BlockID))

# Rename datasets using prepared dictionary

dataset_order <- left_join(dataset_order, block_ids, by=c("BlockID", "Condition"))

dataset_order <- dataset_order %>% select(-BlockID)

```


```{r}

stats_datasets <- dplyr::select(raw_responses, "filename", "Demo-ResponseID", starts_with("Stats-training"), starts_with("Stats-dataset-"))

stats_datasets_tall <- stats_datasets %>% gather("ColName","Value",-c(1,2)) %>% separate(ColName, c("Group","Subgroup","Dataset","Measurement"), sep="[-]", convert=TRUE) %>% select(-Group, -Subgroup) %>% filter(!(Measurement %in% c("order","yn"))) %>% spread(Measurement, Value)

names(stats_datasets_tall)[names(stats_datasets_tall) == "start"] <- "DatasetStartTime"

names(stats_datasets_tall)[names(stats_datasets_tall) == "duration"] <- "DatasetDuration"

stats_datasets_tall <- stats_datasets_tall %>% filter(!is.na(DatasetStartTime), !is.na(DatasetDuration))

#write_csv(stats_datasets_tall, file.path(analysisDataDir,"Stats_Datasets_Tall.csv"))
```

```{r}

# Join dataset order to stats_dataset_tall

conditions <- stats_demo %>% select(`Demo-ResponseID`,`Stats-Group`)

stats_datasets_tall <- left_join(stats_datasets_tall, conditions)

stats_datasets_tall <- left_join(stats_datasets_tall, dataset_order, by=c("Demo-ResponseID","Dataset")) %>% select(-Condition)

stats_datasets_tall$DatasetOrder[stats_datasets_tall$Dataset==0] <- 0

stats_datasets_tall <- full_join(stats_datasets_tall, population_experience)



```

```{r}
# Join dataset order to exp response data

exp_responses_with_order <- left_join(exp_responses_split, dataset_order, by=c("Demo-ResponseID","Condition","Dataset"))

exp_responses_with_order$DatasetOrder <- ifelse(exp_responses_with_order$QType == "Training", 0, exp_responses_with_order$DatasetOrder)

```

```{r}

# Extract question order questions to new table, including RespondentID!

question_order <- dplyr::select(raw_responses, "Demo-ResponseID", starts_with("OrderQuestions"))

# Transpose 12 order questions from wide to tall into two columns - column labels and values

question_order <- question_order %>% gather("Column","Order",-1)

# Remove NAs?

question_order <- question_order %>% filter(!is.na(Order))

# Split column labels to get block and dataset in their own columns

question_order$Column <- sub("OrderQuestionsWithinDataset-","",question_order$Column)

question_order <- question_order %>% separate(Column,c("Condition","Dataset"),sep="[-]", convert=TRUE)

# Split values to get the ordered questions in separate columns

question_order <- question_order %>% separate(Order, c("1","2","3","4","5","6","7","8","9"), sep="[|]")

# Transpose question columns into one column for question name and another for question order

question_order <- question_order %>% gather("TaskOrder","Task",-c(1:3), convert=TRUE)

question_order$Task <- sub("â€”","-",question_order$Task)

question_order <- question_order %>% 
  separate(Task,c("ConditionCopy","DatasetCopy","Task"),sep="[-]") %>%
  select(-ConditionCopy,-DatasetCopy)

# Change LargeClust to LargeClust1

question_order$Task <- sub("LargeClust","LargeClust1",question_order$Task)

# Join question order to exp response data

exp_responses_with_order <- left_join(exp_responses_with_order, question_order, by=c("Demo-ResponseID","Condition","Dataset","Task"))

```


```{r}

new_training_rows <- exp_responses_with_order %>% filter(QType == "Training") %>% select(-TaskOrder) %>% left_join(training_task_order)

exp_responses_with_order <- exp_responses_with_order %>% filter(QType != "Training") %>% bind_rows(new_training_rows)

```

```{r}

# for people in Col, Phr, and Size condition, change the Condition in the Training tasks 
# to match their group
# (these groups saw the same training as the Ctrl condition, so their training rows are labeled Ctrl 
# instead of the correct layout conditions, which is confusing when trying to compare their
# experimental tasks to their training tasks)

exp_responses_with_order <- select(raw_responses, `Demo-ResponseID`, `Stats-Group`) %>% right_join(exp_responses_with_order) %>% mutate(Condition = ifelse(`Stats-Group` == "color","Col",ifelse(`Stats-Group` == "phrasing", "Phr", ifelse(`Stats-Group` == "size", "Siz",Condition)))) %>% select(-`Stats-Group`)

```


## Join response data to answer data

```{r}

# Note: numerical answers should be the same regardless of condition, as should clusters.  

joined_num_ans <- inner_join(exp_responses_with_order, num_ans_lookup, by=c("Dataset","Task"))

joined_num_ans$Response <- type.convert(joined_num_ans$Response)

joined_num_ans <- joined_num_ans %>% select(-Coord)

```

```{r}

# For anything related to clicking (degree and betweenness), 
# correct answer will also depend on condition.

# Need to match up x and y from click data

exp_responses_degree <- filter(exp_responses_with_order, Task == "ClickHighDeg")

exp_responses_degree$Task <- "Click"
  
exp_responses_degree <- exp_responses_degree %>% unite("Task", c("Task","Coord"))

exp_responses_degree$Response <- type.convert(exp_responses_degree$Response)

exp_responses_degree <- separate(exp_responses_degree, Task, c("Task","Attempt"), sep=7, convert=TRUE)

exp_responses_degree_wide <- spread(exp_responses_degree, Task, Response)

joined_degree <- left_join(exp_responses_degree_wide, node_lookup[node_lookup$Task == "ClickHighDeg",], by=c("Condition", "Dataset"))

exp_responses_bc <- filter(exp_responses_with_order, Task == "BC")

exp_responses_bc$Task <- "Click"

exp_responses_bc <- exp_responses_bc %>% unite("Task", c("Task","Coord"))

exp_responses_bc$Response <- type.convert(exp_responses_bc$Response)

exp_responses_bc <- separate(exp_responses_bc, Task, c("Task","Attempt"), sep=7, convert=TRUE)

exp_responses_bc_wide <- spread(exp_responses_bc, Task, Response)

joined_bc <- left_join(exp_responses_bc_wide, node_lookup[node_lookup$Task == "BC",], by=c("Condition","Dataset"))

joined_nodes <- bind_rows(joined_degree, joined_bc)


```

## Grade the questions

```{r}

# normalize numerical answers

normalit<-function(m){
   (m - min(m))/(max(m)-min(m))
}

# there is a slight chance the "correct" answer will differ between conditions, even when it shouldn't,
# because all of the answers were calculated by R from network files, and some of them are slightly imperfect

# TO DO: reconsider - is it too weird to have a different min and max for each condition?

joined_num_ans <- joined_num_ans %>% 
  group_by(Condition, Dataset, Task) %>%
  mutate(MinResp = ifelse(min(Response) < CorrectAnswer, min(Response), CorrectAnswer),
         MaxResp = ifelse(max(Response) > CorrectAnswer, max(Response), CorrectAnswer)) %>%
  ungroup()

joined_num_ans <- joined_num_ans %>%
  mutate(NormResponse=(Response - MinResp)/(MaxResp-MinResp), 
         # take numerical responses within condition/data/task and convert to 0 to 1 scale
         NormCorrectAnswer=(CorrectAnswer - MinResp)/(MaxResp-MinResp)) 
         # convert correct answer to same scale

```


```{r}

# grade the numerical answers

# Calculate LogError, a la Heer & Bostock 2010
# Note: Heer & Bostock add 1/8 to the calculation, but that means that perfectly correct answers
# become -3.  Seems strange.  Changing to add 1, so perfectly correct answers have LogError of 0.

# RawDifference will be negative if response was an underestimate

graded_num_ans <- dplyr::mutate(joined_num_ans, 
                                RawDifference = NormResponse - NormCorrectAnswer, 
                                AbsDifference = abs(NormResponse - NormCorrectAnswer), 
                                Percentage = abs(NormResponse - NormCorrectAnswer)/NormCorrectAnswer,
                                LogError = log10(abs(NormResponse - NormCorrectAnswer) + 1),
                                Underestimated = ifelse(RawDifference < 0,
                                                        "under",
                                                        ifelse(RawDifference == 0, 
                                                               "correct",
                                                               "over")))

# so, max of NormResponse - NormCorrectAnswer is 1, then add 1, then take log base 10? 
# so max LogError is .30, but that's relative to the full range of responses rather than
# the scale of the data

#ggplot(graded_num_ans) + geom_boxplot(aes(y=log2(AbsDifference+1),x=factor(Dataset))) + facet_grid(.~Task)
#ggplot(graded_num_ans) + geom_boxplot(aes(y=log10(AbsDifference+1),x=factor(Dataset))) + facet_grid(.~Task)
```


```{r}

# grade the click answers

# find distance between click responses and all node positions

joined_nodes <- mutate(joined_nodes, Distance = sqrt((Click_X-NodeXAdjusted)^2 + (Click_Y-NodeYAdjusted)^2))

# pick the closest node (node where distance is smallest)

#graded_nodes_top <- group_by(graded_nodes, `Demo-ResponseID`, Condition, Dataset, Task, Attempt) %>% top_n(-1, Distance)

# pick best of nearest 5 nodes, rather than keeping the top 1
# take top 5 closest nodes, as long as they're within 25 pixesl,
# reduce to highest rank, then pick the closest in case of tie

#graded_nodes_best_of_top_5 <- group_by(graded_nodes, `Demo-ResponseID`, Condition, Dataset, Task, Attempt) %>% filter(Distance <= 25) %>% top_n(-5, Distance) %>% top_n(-1, NodeRank) %>% top_n(-1, Distance)

# pick best of the nodes within a 25-pixel-radius buffer

graded_nodes_best_of_25_buffer <- group_by(joined_nodes, `Demo-ResponseID`, Condition, Dataset, Task, Attempt) %>% filter(Distance <= 25) %>% top_n(-1, NodeRank) %>% top_n(-1, Distance)

```

```{r}

# normalize click responses

# there is a slight chance the "correct" answer will differ between conditions, even when it shouldn't,
# because all of the answers were calculated by R from network files, and some of them are slightly imperfect

graded_nodes_best_of_25_buffer <- graded_nodes_best_of_25_buffer %>% 
  group_by(Condition, Dataset, Task) %>%
  mutate(MinResp = ifelse(min(NodeValue) < MaxValue, min(NodeValue), MaxValue),
         MaxResp = ifelse(max(NodeValue) > MaxValue, max(NodeValue), MaxValue)) %>%
  ungroup()

graded_nodes_best_of_25_buffer <- graded_nodes_best_of_25_buffer %>%
  mutate(NormResponse=(NodeValue - MinResp)/(MaxResp-MinResp),
         NormCorrectAnswer=(MaxValue - MinResp)/(MaxResp-MinResp))

```

```{r}

graded_nodes_top <- dplyr::mutate(graded_nodes_best_of_25_buffer, 
                                RawDifference = NormResponse - NormCorrectAnswer, 
                                AbsDifference = abs(NormResponse - NormCorrectAnswer), 
                                Percentage = abs(NormResponse - NormCorrectAnswer)/NormCorrectAnswer,
                                LogError = log10(abs(NormResponse - NormCorrectAnswer) + 1),
                                Underestimated = ifelse(RawDifference < 0,
                                                        "under",
                                                        ifelse(RawDifference == 0, 
                                                               "correct",
                                                               "over")))

#ggplot(graded_nodes_top) + geom_boxplot(aes(y=log2(AbsDifference+1),x=factor(Dataset))) + facet_grid(.~Task)

#ggplot(graded_nodes_top %>% filter(Task == "BC")) + geom_histogram(aes(log2(AbsDifference+1))) + facet_grid(Dataset~Attempt)

#ggplot(graded_nodes_top %>% filter(Task == "BC",Dataset>0)) + geom_line(aes(x=Attempt,y=log2(AbsDifference+1))) + facet_wrap(~Dataset)


```

```{r}

BC_summary <- graded_nodes_top %>% filter(Task == "BC",Dataset>0) %>%
  group_by(`Demo-ResponseID`,Task,Dataset,DatasetOrder) %>%
  summarise(MinLogError=min(LogError), AvgLogErrer=mean(LogError), 
            MinNodeRank=min(NodeRank), AvgNodeRank=mean(NodeRank),
            numAttempts=n())



```

```{r}

# remove all but the single best click for BC

graded_nodes_top_bc <- graded_nodes_top %>% 
  filter(Task=="BC") %>%
  group_by(`Demo-ResponseID`, Condition, Dataset) %>% 
  top_n(-1, NodeRank) %>% top_n(-1, Distance)

# join back to graded nodes

graded_nodes_top <- graded_nodes_top %>% 
  filter(Task=="ClickHighDeg") %>% 
  bind_rows(graded_nodes_top_bc)

```

```{r}

# add cluster confidence rating as a new column, but only fill it in for the "NumClust" task

clust_conf <- filter(exp_responses_split, Task == "ClustConf") %>% select(-Coord)

graded_num_ans <- left_join(graded_num_ans, clust_conf, by = c("Demo-ResponseID", "QType", "Condition", "Dataset")) %>% select(-Task.y)

names(graded_num_ans)[names(graded_num_ans) == 'Response.y'] <- 'ClustConf'

names(graded_num_ans)[names(graded_num_ans) == 'Task.x'] <- 'Task'

names(graded_num_ans)[names(graded_num_ans) == 'Response.x'] <- 'Response'

graded_num_ans[graded_num_ans$Task != "NumClust","ClustConf"] <- NA


```

```{r}

# Creating simpler data frames for analysis

graded_num_ans <- left_join(graded_num_ans, stats_demo)
graded_num_ans <- left_join(graded_num_ans, stats_datasets_tall)

graded_nodes_top <- left_join(graded_nodes_top, stats_demo)
graded_nodes_top <- left_join(graded_nodes_top, stats_datasets_tall)

exp_only <- graded_num_ans %>% filter(QType == "Exp")

num_ans_lim <- graded_num_ans %>% filter(QType == "Exp") %>% select(`Demo-ResponseID`, Condition, Dataset, Task, Response, NormResponse, DatasetOrder, DatasetDuration, DatasetStartTime, TaskOrder, CorrectAnswer, NormCorrectAnswer, RawDifference, AbsDifference, Percentage, LogError, Underestimated, ClustConf, filename, `Stats-Q_TotalDuration`, `Stats-dataset_count`, `Stats-CompensationCondition`, `Stats-OperatingSystem`, `Stats-ScreenResolution`, `Demo-age`, `Demo-gender`, `Demo-lang`, `Demo-educ`, `Demo-educ_TEXT`, `Demo-acfield`, `Demo-acfieldother`, `Demo-dailytech_Computer`, `Demo-dailytech_Tablet`, `Demo-dailytech_SmartPhone`, `Demo-weeklygaming`, `Demo-expdataanal`, `Demo-expdatavis`, `Demo-expreadnetvis`, `Demo-expcreatenetvis`, NetVisExperience)

nodes_lim <- graded_nodes_top %>% filter(QType == "Exp") %>% select(`Demo-ResponseID`, Condition, Dataset, Task, Attempt, NodeValue, NormResponse, NodeRank, MaxNodeRank, DatasetOrder, DatasetDuration, DatasetStartTime, TaskOrder, MaxValue, NormCorrectAnswer, RawDifference, AbsDifference, Percentage, LogError, Underestimated, filename, `Stats-Q_TotalDuration`, `Stats-dataset_count`, `Stats-CompensationCondition`, `Stats-OperatingSystem`, `Stats-ScreenResolution`, `Demo-age`, `Demo-gender`, `Demo-lang`, `Demo-educ`, `Demo-educ_TEXT`, `Demo-acfield`, `Demo-acfieldother`, `Demo-dailytech_Computer`, `Demo-dailytech_Tablet`, `Demo-dailytech_SmartPhone`, `Demo-weeklygaming`, `Demo-expdataanal`, `Demo-expdatavis`, `Demo-expreadnetvis`, `Demo-expcreatenetvis`, NetVisExperience)

nodes_lim <- nodes_lim %>% rename(Response = NodeValue, CorrectAnswer = MaxValue)

all_exp <- bind_rows(num_ans_lim, nodes_lim)

all_exp <- all_exp %>% separate(`Stats-ScreenResolution`, c("StatsScreenWidth","StatsScreenHeight"), sep="x", convert=TRUE) %>% mutate(StatsNumPixels = StatsScreenWidth * StatsScreenHeight)

all_exp <- all_exp %>% arrange(`Demo-ResponseID`,DatasetOrder,TaskOrder,Attempt) %>% rowid_to_column("QuestionOrder")

# add in properties of networks

network_props <- num_ans_lookup %>% spread(Task, CorrectAnswer)

all_exp <- left_join(all_exp, network_props)

```

## Omit participants based on response patterns 

```{r}

# remove people who finished but had short times and high error; assume these are MTurkers

total_duration <- stats_demo %>% select(`Demo-ResponseID`,`Stats-Q_TotalDuration`)

avg_error <- graded_num_ans %>% group_by(`Demo-ResponseID`) %>% summarise(num=n(),avgError=mean(LogError),sd=sd(LogError))

avg_error_time <- full_join(avg_error, total_duration) %>% mutate(red_flag = if_else(num>=20 & ntile(`Stats-Q_TotalDuration`,5) < 2 & ntile(avgError,5) > 4,TRUE,FALSE))

table(avg_error_time$red_flag)

red_flag_users <- avg_error_time %>% filter(red_flag == TRUE) %>% select(`Demo-ResponseID`)

# another red flag would be clicking at random; currently, the click data finds the best node
# within a 25-pixel radius, so clicking in a blank area of the graph will already get filtered
# out

# this doesn't currently test for regular clicks in areas that are obviously terrible, but
# hopefully people who would do that are caught with the other methods

```


```{r}

# Omit based on number of tasks completed
# choosing 8 as minimum because that is a full dataset

drop_out_users <- all_exp %>% mutate(DataTask=paste0(Dataset,Task)) %>% group_by(`Demo-ResponseID`) %>% summarise(NumData=n_distinct(Dataset),NumTasks=n_distinct(DataTask)) %>% filter(NumTasks<8) %>% select(`Demo-ResponseID`)


all_omit_users <- bind_rows(red_flag_users, drop_out_users) %>% distinct()

```


```{r}

stats_demo <- stats_demo %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

stats_datasets_tall <- stats_datasets_tall %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

exp_responses_with_order <- exp_responses_with_order %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

graded_num_ans <- graded_num_ans %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

graded_nodes_top <- graded_nodes_top %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

BC_summary <- BC_summary %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

exp_only <- exp_only %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

all_exp <- all_exp %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

#graphics <- graphics %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

#layout_all <- layout_all %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

#layout_fac_1_7_9 <- layout_fac_1_7_9 %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

#layout_mturk_all <- layout_mturk_all %>% filter(!(`Demo-ResponseID` %in% all_omit_users$`Demo-ResponseID`))

```

```{r}
# summarize final number of users

num_users <- all_exp %>% group_by(NetVisExperience,`Demo-ResponseID`,Condition,Dataset) %>% summarise()

table(num_users$Dataset, num_users$Condition, num_users$NetVisExperience)

```

```{r, eval=FALSE}

graphics <- all_exp %>% filter(Condition %in% c("Ctrl","Phr","Siz","Col"), filename %in% c("MTurkFinal","MTurkPilot3")) 
  
layout_fac_1_7_9 <- all_exp %>% filter(Condition %in% c("Ctrl","Fru","Ord","Cir"), Dataset %in% c(1, 7, 9)) 
layout_mturk_all <- all_exp %>% filter(Condition %in% c("Ctrl","Fru","Ord","Cir"), filename %in% c("MTurkFinal","MTurkPilot3"))

layout_all <- all_exp %>% filter(Condition %in% c("Ctrl","Fru","Ord","Cir"))


```

## Write analysis files

```{r}

#write_csv(stats_demo, file.path(analysisDataDir,"Stats_Demo.csv"))
saveRDS(stats_demo, file.path(analysisDataDir,"Stats_Demo.rds"))

#write_csv(stats_datasets_tall, file.path(analysisDataDir,"Stats_Datasets_Tall.csv"))
saveRDS(stats_datasets_tall, file.path(analysisDataDir,"Stats_Datasets_Tall.rds"))

#write_csv(exp_responses_with_order, file.path(analysisDataDir,"CombinedResponsesWithOrder.csv"))
saveRDS(exp_responses_with_order, file.path(analysisDataDir,"CombinedResponsesWithOrder.rds"))

#write_csv(graded_num_ans, file.path(analysisDataDir,"GradedNumAnswers.csv"))
saveRDS(graded_num_ans, file.path(analysisDataDir,"GradedNumAnswers.rds"))

#write_csv(graded_nodes_top, file.path(analysisDataDir,"GradedNodes.csv"))
saveRDS(graded_nodes_top, file.path(analysisDataDir,"GradedNodes.rds"))

#write_csv(BC_summary, file.path(analysisDataDir,"BCSummary.csv"))
saveRDS(BC_summary, file.path(analysisDataDir,"BCSummary.rds"))

#write_csv(exp_only, file.path(analysisDataDir,"ExpOnly.csv"))
saveRDS(exp_only, file.path(analysisDataDir,"ExpOnly.rds"))

#write_csv(all_exp, file.path(analysisDataDir,"AllExperimental.csv"))
saveRDS(all_exp, file.path(analysisDataDir,"AllExperimental.rds"))

#write_csv(graphics, file.path(analysisDataDir,"Graphics.csv"))
#saveRDS(graphics, file.path(analysisDataDir,"Graphics.rds"))

#write_csv(layout_all, file.path(analysisDataDir,"LayoutAll.csv"))
#saveRDS(layout_all, file.path(analysisDataDir,"LayoutAll.rds"))

#write_csv(layout_fac_1_7_9, file.path(analysisDataDir,"LayoutFac.csv"))
#saveRDS(layout_fac_1_7_9, file.path(analysisDataDir,"LayoutFac.rds"))

#write_csv(layout_mturk_all, file.path(analysisDataDir,"LayoutTurk.csv"))
#saveRDS(layout_mturk_all, file.path(analysisDataDir,"LayoutTurk.rds"))

```
